# ğŸ§  LaminaAI â€“ Your Local AI Companion

**LaminaAI** is a developer-friendly framework for building local, conversational AI assistants and companions with **long-term memory**, **context awareness**, and a **customizable personality**.

> Ships with **Lamina** â€“ a warm and playful AI persona â€“ as the default character.

---

## âœ¨ Features

* ğŸ§  **Long-Term Memory** using local ChromaDB (no cloud)
* ğŸ—ï¸ **Message Summarization** and semantic embedding
* ğŸ” **Context Recall** via vector search (RAG-style)
* ğŸ’¬ **Short-Term Memory Queue** (recent chat context)
* âš™ï¸ Built with **Node.js**, **TypeScript**, **FastAPI, ChromaDB (Python)**, and **Ollama LLM**
* ğŸ”„ Easy to switch models via [Ollama](https://ollama.com/) (e.g. Hermes, Mistral, etc.)

---

## ğŸš€ Getting Started

### ğŸ–¥ Requirements

* Node.js â‰¥ 18
* Python â‰¥ 3.10
* [Ollama](https://ollama.com/) installed locally

---

### ğŸ“¦ Installation

```bash
# Clone the project
git clone https://github.com/rahul-lamkhade/LaminaAI.git
cd LaminaAI

# Set up orchestrator app
cd services/orchestrator
npm install

# Set up Python memory service
cd services/memory-api
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# Now start all services
# in terminal
ollama serve
# in services/memory-api
uvicorn src.vector_store:app --reload --reload-dir=src
# in services/orchestrator
npx ts-node src/index.ts
```

---

### ğŸ’  How It Works

* Recent messages are kept in a short-term memory queue (up to 10).
* Older chats are summarized and stored in **ChromaDB** with metadata.
* On each new message:

  * Fetch recent summaries + active short-term queue
  * Perform semantic search if trigger words match (e.g., *"remember"*)
* Final prompt is sent to the LLM (via Ollama) for response generation.

---

### ğŸ”Œ API Endpoints (Python Memory Server)

| Endpoint             | Purpose                         |
| -------------------- | ------------------------------- |
| `POST /memory/add`   | Store a summarized memory       |
| `GET /memory/recent` | Get last N memory chunks        |
| `POST /query`        | Semantic memory search (vector) |

---

### ğŸ§  Character: Lamina

Lamina is a warm, emotionally intelligent AI companion. You can customize her personality, tone, and prompt behavior in:

```ts
services/orchestrator/src/agents/lamina.ts
```

---

### ğŸ§± Use Cases

* Personal AI assistant (local & private)
* Mental health journaling support
* Story-based NPCs or game characters
* Offline companions for low-resource environments

---

### ğŸŒ± Contributing

This project is modular and designed to be extended or embedded in other tools. Fork it, remix it, or send a PR.

---

### ï¿½ï¿½ License

MIT â€” open-source and free to use for personal or commercial projects.

---

### ğŸ“² Coming Soon

* ğŸ³ Docker integration
* ğŸ§˜ Emotional state tracking
* ğŸ“± Mobile UI (Ionic + Angular)
* ğŸ§ Voice integration (TTS/STT)
* ğŸŒ Web-based UI